{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ,os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "x_train = np.load(\"sst-train-x.npy\",allow_pickle=True) \n",
    "y_train = np.load(\"sst-train-y.npy\",allow_pickle=True)\n",
    "x_test  = np.load(\"sst-test-x.npy\",allow_pickle=True)\n",
    "y_test  = np.load(\"sst-test-y.npy\",allow_pickle=True)\n",
    "x_train = x_train.reshape(67349,1,1,700)\n",
    "x_test = x_test.reshape(872,1,1,700)\n",
    "\n",
    "\n",
    "import sys ,os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import math\n",
    "# 激活函数\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    np.where(condition,x,y) 当条件成立时where方法返回x，当条件不成立时where返回y\n",
    "    return x>0  ,x\n",
    "           x<=0 ,0\n",
    "    \"\"\"\n",
    "    x_1 = np.where(x < 0 ,0,x)\n",
    "    return x_1\n",
    "\n",
    "def softmax(x):\n",
    "    exp_a = np.exp(x-np.max(x,axis = 1,keepdims=True)) # 行\n",
    "    sum_exp_a = np.sum(exp_a,axis = 1,keepdims=True)\n",
    "    ret = exp_a/sum_exp_a\n",
    "    return ret\n",
    "\n",
    "def cross_entropy(y,y_hat):\n",
    "    return -np.sum(y*np.log(y_hat + np.exp(-7)))\n",
    "\n",
    "def get_im2col_indices(x_shape, field_height, field_width, stride=1,padding=0):\n",
    "  # First figure out what the size of the output should be\n",
    "  N, C, H, W = x_shape\n",
    "  assert (H + 2 * padding - field_height) % stride == 0\n",
    "  assert (W + 2 * padding - field_height) % stride == 0\n",
    "  out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "  out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "  i0 = np.repeat(np.arange(field_height), field_width)\n",
    "  i0 = np.tile(i0, C)\n",
    "  i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "  j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "  j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "  i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "  j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "  k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "  return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col(x, field_height, field_width, stride=1 ,padding=0):\n",
    "  \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "  # Zero-pad the input\n",
    "  p = padding\n",
    "  x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "  k, i, j = get_im2col_indices(x.shape, field_height, field_width, stride, padding)\n",
    "\n",
    "  cols = x_padded[:, k, i, j]\n",
    "  C = x.shape[1]\n",
    "  cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "  return cols\n",
    "\n",
    "\n",
    "def col2im(cols, x_shape, field_height, field_width, stride=1, padding=0):\n",
    "  \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "  N, C, H, W = x_shape\n",
    "  H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "  x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "  k, i, j = get_im2col_indices(x_shape, field_height, field_width, stride, padding)\n",
    "  cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "  cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "  np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "  if padding == 0:\n",
    "    return x_padded\n",
    "  return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "pass\n",
    "\n",
    "\n",
    "# 卷积层\n",
    "class Convolution:\n",
    "    def __init__(self, w, bias, stride = 1, pad = 0):\n",
    "        self.x = None\n",
    "        self.w = w\n",
    "        self.b  = bias\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def conv(self, x, w, bias, stride, pad):\n",
    "        #获取输入x、卷积核的维度\n",
    "        FN,NC,FH,FW = w.shape\n",
    "        m,nc_in, h_in,w_in = x.shape\n",
    "    \n",
    "        #计算输出的height和width\n",
    "        h_out = int((h_in+2*pad-FH)/stride + 1)\n",
    "        w_out = int((w_in+2*pad-FW)/stride + 1)\n",
    "\n",
    "        #将x按照行展开（m*h_out*w_out,FH*FW*NC)\n",
    "        #此时不同的sample同一个channel的值是相邻的\n",
    "        x_col = im2col(x,FH,FW,stride,pad).T\n",
    "        \n",
    "        #将卷积核按列展开(FH*FW*C,FN)\n",
    "        w_col = w.reshape(FN,FH*FW*NC).T\n",
    "\n",
    "        #计算卷积 (m*h_out*w_out,FN)->T->reshape\n",
    "        out = (np.dot(x_col,w_col)+bias)\n",
    "       \n",
    "        #将batch提到最高维度（共三维）\n",
    "        out_1 = np.zeros((m,h_out*w_out,FN))\n",
    "        for i in range(m*h_out*w_out):\n",
    "            out_1[i%m,i//m,:] = out[i,:]\n",
    "        #最后调整维度\n",
    "        out = out_1.transpose((0,2,1)).reshape(m,FN,h_out,w_out)\n",
    "        return out,x_col\n",
    "    \n",
    "    def central_symmetry(self,w):\n",
    "        w_r=w.transpose((2,3,0,1))\n",
    "        w_r = np.flipud(np.fliplr(w_r))\n",
    "        w_r=w_r.transpose((3,2,0,1))# 2,3,0,1 交换前两个，因为这时候的卷积核个数变了\n",
    "        return w_r\n",
    "\n",
    "        \n",
    "    def forward (self,x):\n",
    "        self.x = x # x并不固定，不在初始化的时候直接赋值\n",
    "        out,x_col = Convolution.conv(self,x,self.w,self.b,self.stride,self.pad)\n",
    "        self.out_shape = out.shape\n",
    "        self.x_col = x_col\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout, learning_rate = 0.01):\n",
    "        # shape为：（h_out*w_out*m，nc_out）不同batch的相邻的channel相邻\n",
    "        dout_col = dout.transpose(1,0,2,3).reshape(self.w.shape[0],-1).T\n",
    "\n",
    "        #dx dx是正确的，但是dx应该在w更新之前更新\n",
    "        w_r = Convolution.central_symmetry(self,self.w)\n",
    "        bias_z = np.zeros((1,self.w.shape[1]))\n",
    "        dx,_ = Convolution.conv(self,dout,w_r,bias_z,1,w_r.shape[3]- 1) # padding\n",
    "\n",
    "        dw = dout.transpose(1, 2, 3, 0).reshape(self.w.shape[0], -1).dot(self.x_col).reshape(self.w.shape)\n",
    "        self.w = self.w - learning_rate*dw /float(dout.shape[0])\n",
    "\n",
    "        db = np.sum(dout_col,axis=0,keepdims=True).reshape(1,-1)\n",
    "        self.b = self.b - learning_rate*db /float(dout.shape[0])\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "# 池化层\n",
    "class Pooling:\n",
    "    def __init__(self,pool_h,pool_w,stride = 1,pad = 0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        self.x_col = None\n",
    "        \n",
    "    def forward ( self,x) :\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        h_out = int(1+(H-self.pool_h)/self.stride)\n",
    "        w_out = int(1+(W-self.pool_w)/self.stride)\n",
    "\n",
    "        # 展开 (N*h_out*w_out,H*W*C)\n",
    "        x_col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "\n",
    "        # 以N为最高维reshape\n",
    "        x_col_r = np.zeros((N,h_out*w_out,self.pool_h*self.pool_w*C))\n",
    "        for i in range(N*h_out*w_out):\n",
    "            x_col_r[i%N,i//N,:]=x_col[:,i] # 用的是x_col的转置\n",
    "        x_col = x_col_r.transpose(0,2,1)\n",
    "\n",
    "        # 以channel为最高维reshape\n",
    "        x_col = x_col.reshape(N,C,self.pool_w*self.pool_h,h_out*w_out).transpose(0,1,3,2)\n",
    "        self.x_col = x_col \n",
    "        # 最大值\n",
    "        arg_max = np.argmax(x_col,axis=3)\n",
    "        self.arg_max = arg_max \n",
    "        out = np.max(x_col,axis=3)\n",
    "        out =out.reshape(N,C,h_out,w_out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self ,dout):\n",
    " \n",
    "        N, C, H, W = self.x.shape\n",
    "        # 根据arg_max生成二维damx\n",
    "        pool_size = self.pool_h*self.pool_w\n",
    "        dmax = np.zeros((dout.size,pool_size)) \n",
    "        dmax[np.arange(self.arg_max.size),self.arg_max.flatten()] = dout.flatten()\n",
    "\n",
    "        # dmax矩阵中，不同batch分开存放，为使用col2im将其变为在一起（类似im2col结果）\n",
    "        dmax_1 = np.zeros((dmax.shape))\n",
    "        size = dmax.shape[0] // N\n",
    "        for i in range(dmax.shape[0]):\n",
    "            dmax_1[(i%size) * N + i//size,:] = dmax[i,:]\n",
    "        dmax = dmax_1\n",
    "\n",
    "        # 为和im2col的情况一样，将二维d_max中按列连接的不同channel数据分开放在不同列中\n",
    "        dmax_2 = np.zeros((dmax.shape[0] //C , dmax.shape[1] *C))\n",
    "        for i in range(C):\n",
    "            dmax_2[:,i*dmax.shape[1]:(i+1)*dmax.shape[1]] = dmax[i*dmax_2.shape[0]:(i+1)*dmax_2.shape[0],:]\n",
    "        dmax = dmax_2.T # 注意要转置\n",
    "        \n",
    "        # 获得dx\n",
    "        dx = col2im(dmax,self.x.shape,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        return dx\n",
    "\n",
    "\n",
    "# relu层\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self ,x):\n",
    "        self.mask = x <= 0 # 记录x<0的位置\n",
    "        out = x\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout \n",
    "        return dx\n",
    "\n",
    "\n",
    "# softmax层\n",
    "class SoftMax:\n",
    "    def __init__ (self):\n",
    "        self.y_hat = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        self.y_hat = softmax(x)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def backward(self,labels):\n",
    "        m = labels.shape[0]\n",
    "        dx = self.y_hat.copy()\n",
    "        dx = dx - labels\n",
    "        return dx\n",
    "\n",
    "\n",
    "# affine fc层\n",
    "class Affine:\n",
    "    def __init__(self,w,b):\n",
    "        self.w = w # shape is (n_x,n_unit)\n",
    "        self.b  = b  # shape is(1,n_unit)\n",
    "        self.X = None\n",
    "        self.origin_x_shape = None\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "        self.out_shape =None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.origin_x_shape = x.shape \n",
    "        # 将四维的xreshape成为2维的，batch在第一维\n",
    "        self.x = x.reshape(x.shape[0],-1)#(m,n)\n",
    "        out =  np.dot(self.x, self.w)+self.b\n",
    "        self.out_shape = out.shape\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout,learning_rate = 0.01):        \n",
    "        m = float(self.x.shape[0])\n",
    "        \n",
    "        self.dw = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0,keepdims=True)\n",
    "        \n",
    "        dx = np.dot(dout,self.w.T)\n",
    "        dx = dx.reshape(self.origin_x_shape) # 保持与之前的x一样的shape\n",
    "        \n",
    "        #更新W和b\n",
    "        self.w = self.w-learning_rate*self.dw/m\n",
    "        self.b = self.b - learning_rate*self.db/m\n",
    "        return dx\n",
    "\n",
    "# model\n",
    "class SimpleConvNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.Y= None\n",
    "        self.layers = []\n",
    "\n",
    "    def add_conv_layer(self, core_num, cha_num, f, stride=1, pad=0):\n",
    "        # 初始化W，b\n",
    "        w = np.random.randn(core_num, cha_num, f, f)*0.1\n",
    "        bias = np.zeros((1, core_num))\n",
    "        # 卷积层\n",
    "        Conv = Convolution(w, bias, stride=stride, pad=pad)\n",
    "        return Conv\n",
    "\n",
    "    def add_maxpool_layer(self, pool_shape, stride=1, pad=0):\n",
    "        pool_h, pool_w = pool_shape\n",
    "        pool = Pooling(pool_h, pool_w, stride=stride, pad=pad)\n",
    "        \n",
    "        return pool\n",
    "    \n",
    "    def add_affine(self, m, n_units):\n",
    "        w= np.random.randn(m, n_units)*0.1\n",
    "        \n",
    "        b = np.zeros((1, n_units))\n",
    "        \n",
    "        fc_layer = Affine(w,b)\n",
    "        \n",
    "        return fc_layer\n",
    "    \n",
    "    def add_relu(self):\n",
    "        relu_layer =  Relu()\n",
    "        return relu_layer\n",
    "    \n",
    "    \n",
    "    def add_softmax(self):\n",
    "        softmax_layer = SoftMax()\n",
    "        return softmax_layer\n",
    "    \n",
    "    #计算卷积或池化后的H和W\n",
    "    def cacl_out_hw(self,HW,f,stride = 1,pad = 0): # 卷积核的大小\n",
    "        return (HW+2*pad - f)/stride+1\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def init_model(self,train_X,n_classes,continue_train = False):\n",
    "\n",
    "        save_index = []\n",
    "        N,C,H,W = train_X.shape\n",
    "       \n",
    "        #Affinen\n",
    "        n_units=700\n",
    "        fc_layer = self.add_affine(n_units,256)\n",
    "        self.layers.append(fc_layer)\n",
    "        save_index.append(0)\n",
    "\n",
    "        #Affinen\n",
    "        n_units=256\n",
    "        fc_layer = self.add_affine(n_units,64)\n",
    "        self.layers.append(fc_layer)\n",
    "        save_index.append(1)\n",
    "\n",
    "        #Affinen\n",
    "        n_units=64\n",
    "        fc_layer = self.add_affine(n_units,2)\n",
    "        self.layers.append(fc_layer)\n",
    "        save_index.append(2)\n",
    "\n",
    "        #SoftMax\n",
    "        softmax_layer = self.add_softmax()\n",
    "        self.layers.append(softmax_layer)\n",
    "        self.save_index = save_index\n",
    "        if continue_train :\n",
    "            save=np.load(\"D:\\\\torch_work\\\\cnn_param.npy\",allow_pickle= True)\n",
    "            i = 0\n",
    "            for index in save_index:\n",
    "                self.layers[index].w = save[i]\n",
    "                i = i+1\n",
    "                self.layers[index].b = save[i]\n",
    "                i = i+1\n",
    "        \n",
    "    def forward_progation(self,train_X, print_out = False): # 优化 用一个循环就可以了 \n",
    "        N,C,H,W = train_X.shape\n",
    "        index = 0\n",
    "        X = train_X\n",
    "        for index in range(len(self.layers)):\n",
    "            layer = self.layers[index]\n",
    "            X = layer.forward(X)\n",
    "            if print_out:\n",
    "                print(str(index)+\"次操作之后：\"+str(X.shape))\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    def back_progation(self,train_y,learning_rate = 0.01):\n",
    "        save =[]\n",
    "        dout = train_y\n",
    "        for index in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[index]\n",
    "            dout = layer.backward(dout)\n",
    "        for index in self.save_index:\n",
    "            save.append(self.layers[index].w)\n",
    "            save.append(self.layers[index].b)\n",
    "        save = np.array(save)\n",
    "        np.save(\"D:\\\\torch_work\\\\cnn_param\",save)\n",
    "        \n",
    "      \n",
    "    def get_minibatch(self,batch_data,minibatch_size,num):\n",
    "        m_examples = batch_data.shape[0]\n",
    "        minibatches = math.ceil( m_examples / minibatch_size) # “向上取整”\n",
    " \n",
    "        if(num < minibatches):\n",
    "            return batch_data[num*minibatch_size:(num+1)*minibatch_size]\n",
    "        else:\n",
    "            return batch_data[num*minibatch_size:m_examples]\n",
    "    \n",
    "    \n",
    "    def optimize(self,train_x, train_y,test_x, test_y, minibatch_size,learning_rate=0.01,num_iters=500):\n",
    "\n",
    "        m = train_x.shape[0]\n",
    "        num_batches  = math.ceil(m / minibatch_size)\n",
    "\n",
    "        train_losss = [] # 训练集上的loss\n",
    "        test_losss  = []\n",
    "        train_accs  = [] # 训练集上的acc\n",
    "        test_accs   = []\n",
    "        \n",
    "        for iteration in range(num_iters):\n",
    "            for batch_num in range(num_batches):\n",
    "                minibatch_x = self.get_minibatch(train_x,minibatch_size,batch_num)\n",
    "                minibatch_y = self.get_minibatch(train_y,minibatch_size,batch_num)\n",
    "                # 前向传播\n",
    "                A = self.forward_progation(minibatch_x)\n",
    "                # 反向传播\n",
    "                self.back_progation(minibatch_y,learning_rate)\n",
    "                acc = (np.argmax(A,axis = 1) == np.argmax(minibatch_y, axis = 1)).sum() / minibatch_size\n",
    "                loss = cross_entropy(minibatch_y, A) / minibatch_size\n",
    "                print(\"iter : %d, batch : %d, acc %g, loss %g \"%(iteration, batch_num, acc, loss))\n",
    "            y_pred     = self.forward_progation(train_x)\n",
    "            y_pred_t   = self.forward_progation(test_x)\n",
    "            train_acc  = (np.argmax(y_pred,axis = 1) == np.argmax(train_y, axis = 1)).sum() / train_x.shape[0]\n",
    "            test_acc   = (np.argmax(y_pred_t,axis = 1) == np.argmax(test_y, axis = 1)).sum() / test_x.shape[0]\n",
    "            train_loss = cross_entropy(train_y, y_pred) / train_x.shape[0]\n",
    "            test_loss  = cross_entropy(test_y, y_pred_t) / test_x.shape[0]\n",
    "            print(f\"\\nTurn: {iteration} -->\\n\"\n",
    "              f\"Train Loss: {train_loss}\\n\"\n",
    "              f\"Test Loss: {test_loss}\\n\"\n",
    "              f\"Train Accuracy: {train_acc}\\n\"\n",
    "              f\"Test Accuracy: {test_acc}\\n\")\n",
    "            train_losss.append(train_loss)\n",
    "            test_accs.append(test_acc)\n",
    "            train_accs.append(train_acc)\n",
    "            test_losss.append(test_loss)\n",
    "            \n",
    "        #画出损失函数图\n",
    "        plt.figure(1)\n",
    "        plt.plot(train_losss,color='b')\n",
    "        plt.plot(test_losss,color='r')\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.savefig('cnn_loss_aat.png')\n",
    "\n",
    "        #acc图\n",
    "        plt.figure(2)\n",
    "        plt.plot(train_accs,color='b')\n",
    "        plt.plot(test_accs,color='r')\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"acc\")\n",
    "        plt.savefig('cnn_acc_aat.png')\n",
    "        \n",
    "       \n",
    "    def predicate(self, train_X):\n",
    "        \"\"\"\n",
    "        预测\n",
    "        \"\"\"\n",
    "        logits = self.forward_progation(train_X)\n",
    "        one_hot = np.zeros_like(logits) # 生成logits大小的zero矩阵\n",
    "        one_hot[range(train_X.shape[0]),np.argmax(logits,axis=1)] = 1\n",
    "        return one_hot   \n",
    "\n",
    "    def fit(self,train_X, train_y, test_x, test_y):\n",
    "        \"\"\"\n",
    "        训练\n",
    "        \"\"\"\n",
    "        self.X = train_X\n",
    "        self.Y = train_y\n",
    "        n_y = train_y.shape[1]\n",
    "        m = train_X.shape[0]\n",
    "        \n",
    "        #初始化模型\n",
    "        self.init_model(train_X,n_classes=n_y,continue_train = False)\n",
    "        self.optimize(train_X, train_y, test_x, test_y,minibatch_size=600,learning_rate=0.01,num_iters=100)\n",
    "        \n",
    "        logits = self.predicate(train_X)\n",
    "        \n",
    "        accuracy = np.sum(np.argmax(logits,axis=1) == np.argmax(train_y,axis=1))/m\n",
    "        print(\"训练集的准确率为：%g\" %(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter : 0, batch : 0, acc 0.506667, loss 0.736045 \n",
      "iter : 0, batch : 1, acc 0.48, loss 0.723796 \n",
      "iter : 0, batch : 2, acc 0.51, loss 0.723373 \n",
      "iter : 0, batch : 3, acc 0.468333, loss 0.745038 \n",
      "iter : 0, batch : 4, acc 0.443333, loss 0.74838 \n",
      "iter : 0, batch : 5, acc 0.491667, loss 0.72886 \n",
      "iter : 0, batch : 6, acc 0.453333, loss 0.736441 \n",
      "iter : 0, batch : 7, acc 0.473333, loss 0.739201 \n",
      "iter : 0, batch : 8, acc 0.451667, loss 0.749515 \n",
      "iter : 0, batch : 9, acc 0.488333, loss 0.73592 \n",
      "iter : 0, batch : 10, acc 0.475, loss 0.727829 \n",
      "iter : 0, batch : 11, acc 0.498333, loss 0.718922 \n",
      "iter : 0, batch : 12, acc 0.473333, loss 0.73789 \n",
      "iter : 0, batch : 13, acc 0.503333, loss 0.709388 \n",
      "iter : 0, batch : 14, acc 0.486667, loss 0.732583 \n",
      "iter : 0, batch : 15, acc 0.485, loss 0.72932 \n",
      "iter : 0, batch : 16, acc 0.493333, loss 0.728717 \n",
      "iter : 0, batch : 17, acc 0.463333, loss 0.737816 \n",
      "iter : 0, batch : 18, acc 0.543333, loss 0.691613 \n",
      "iter : 0, batch : 19, acc 0.521667, loss 0.70701 \n",
      "iter : 0, batch : 20, acc 0.485, loss 0.721967 \n",
      "iter : 0, batch : 21, acc 0.511667, loss 0.710305 \n",
      "iter : 0, batch : 22, acc 0.533333, loss 0.704283 \n",
      "iter : 0, batch : 23, acc 0.495, loss 0.726608 \n",
      "iter : 0, batch : 24, acc 0.533333, loss 0.712169 \n",
      "iter : 0, batch : 25, acc 0.515, loss 0.709343 \n",
      "iter : 0, batch : 26, acc 0.513333, loss 0.71011 \n",
      "iter : 0, batch : 27, acc 0.5, loss 0.720308 \n",
      "iter : 0, batch : 28, acc 0.463333, loss 0.72442 \n",
      "iter : 0, batch : 29, acc 0.521667, loss 0.702545 \n",
      "iter : 0, batch : 30, acc 0.526667, loss 0.706901 \n",
      "iter : 0, batch : 31, acc 0.53, loss 0.69037 \n",
      "iter : 0, batch : 32, acc 0.533333, loss 0.69308 \n",
      "iter : 0, batch : 33, acc 0.53, loss 0.695039 \n",
      "iter : 0, batch : 34, acc 0.516667, loss 0.702926 \n",
      "iter : 0, batch : 35, acc 0.536667, loss 0.704675 \n",
      "iter : 0, batch : 36, acc 0.53, loss 0.700687 \n",
      "iter : 0, batch : 37, acc 0.516667, loss 0.714936 \n",
      "iter : 0, batch : 38, acc 0.545, loss 0.703656 \n",
      "iter : 0, batch : 39, acc 0.536667, loss 0.687943 \n",
      "iter : 0, batch : 40, acc 0.521667, loss 0.703734 \n",
      "iter : 0, batch : 41, acc 0.523333, loss 0.693097 \n",
      "iter : 0, batch : 42, acc 0.523333, loss 0.691002 \n",
      "iter : 0, batch : 43, acc 0.521667, loss 0.693205 \n",
      "iter : 0, batch : 44, acc 0.535, loss 0.695819 \n",
      "iter : 0, batch : 45, acc 0.563333, loss 0.675995 \n",
      "iter : 0, batch : 46, acc 0.52, loss 0.688826 \n",
      "iter : 0, batch : 47, acc 0.533333, loss 0.704112 \n",
      "iter : 0, batch : 48, acc 0.558333, loss 0.693932 \n",
      "iter : 0, batch : 49, acc 0.555, loss 0.68105 \n",
      "iter : 0, batch : 50, acc 0.545, loss 0.688187 \n",
      "iter : 0, batch : 51, acc 0.538333, loss 0.680458 \n",
      "iter : 0, batch : 52, acc 0.508333, loss 0.693082 \n",
      "iter : 0, batch : 53, acc 0.591667, loss 0.666974 \n",
      "iter : 0, batch : 54, acc 0.528333, loss 0.690574 \n",
      "iter : 0, batch : 55, acc 0.578333, loss 0.676031 \n",
      "iter : 0, batch : 56, acc 0.566667, loss 0.672482 \n",
      "iter : 0, batch : 57, acc 0.548333, loss 0.684233 \n",
      "iter : 0, batch : 58, acc 0.536667, loss 0.689342 \n",
      "iter : 0, batch : 59, acc 0.56, loss 0.681385 \n",
      "iter : 0, batch : 60, acc 0.578333, loss 0.657107 \n",
      "iter : 0, batch : 61, acc 0.536667, loss 0.685832 \n",
      "iter : 0, batch : 62, acc 0.59, loss 0.666449 \n",
      "iter : 0, batch : 63, acc 0.555, loss 0.679899 \n",
      "iter : 0, batch : 64, acc 0.595, loss 0.669901 \n",
      "iter : 0, batch : 65, acc 0.563333, loss 0.674923 \n",
      "iter : 0, batch : 66, acc 0.563333, loss 0.678819 \n",
      "iter : 0, batch : 67, acc 0.518333, loss 0.693401 \n",
      "iter : 0, batch : 68, acc 0.563333, loss 0.679157 \n",
      "iter : 0, batch : 69, acc 0.545, loss 0.670058 \n",
      "iter : 0, batch : 70, acc 0.553333, loss 0.673235 \n",
      "iter : 0, batch : 71, acc 0.605, loss 0.672884 \n",
      "iter : 0, batch : 72, acc 0.566667, loss 0.665635 \n",
      "iter : 0, batch : 73, acc 0.555, loss 0.680085 \n",
      "iter : 0, batch : 74, acc 0.583333, loss 0.667076 \n",
      "iter : 0, batch : 75, acc 0.583333, loss 0.67186 \n",
      "iter : 0, batch : 76, acc 0.575, loss 0.67309 \n",
      "iter : 0, batch : 77, acc 0.575, loss 0.659856 \n",
      "iter : 0, batch : 78, acc 0.546667, loss 0.682701 \n",
      "iter : 0, batch : 79, acc 0.633333, loss 0.656264 \n",
      "iter : 0, batch : 80, acc 0.556667, loss 0.681591 \n",
      "iter : 0, batch : 81, acc 0.583333, loss 0.663917 \n",
      "iter : 0, batch : 82, acc 0.606667, loss 0.659925 \n",
      "iter : 0, batch : 83, acc 0.555, loss 0.67574 \n",
      "iter : 0, batch : 84, acc 0.58, loss 0.666819 \n",
      "iter : 0, batch : 85, acc 0.61, loss 0.660432 \n",
      "iter : 0, batch : 86, acc 0.595, loss 0.671304 \n",
      "iter : 0, batch : 87, acc 0.558333, loss 0.674622 \n",
      "iter : 0, batch : 88, acc 0.601667, loss 0.656216 \n",
      "iter : 0, batch : 89, acc 0.581667, loss 0.666562 \n",
      "iter : 0, batch : 90, acc 0.586667, loss 0.674461 \n",
      "iter : 0, batch : 91, acc 0.59, loss 0.662911 \n",
      "iter : 0, batch : 92, acc 0.586667, loss 0.660858 \n",
      "iter : 0, batch : 93, acc 0.581667, loss 0.654224 \n",
      "iter : 0, batch : 94, acc 0.526667, loss 0.680926 \n",
      "iter : 0, batch : 95, acc 0.561667, loss 0.678357 \n",
      "iter : 0, batch : 96, acc 0.63, loss 0.637719 \n",
      "iter : 0, batch : 97, acc 0.575, loss 0.676242 \n",
      "iter : 0, batch : 98, acc 0.583333, loss 0.657827 \n",
      "iter : 0, batch : 99, acc 0.578333, loss 0.660529 \n",
      "iter : 0, batch : 100, acc 0.603333, loss 0.647832 \n",
      "iter : 0, batch : 101, acc 0.613333, loss 0.644138 \n",
      "iter : 0, batch : 102, acc 0.578333, loss 0.665979 \n",
      "iter : 0, batch : 103, acc 0.61, loss 0.65315 \n",
      "iter : 0, batch : 104, acc 0.6, loss 0.659053 \n",
      "iter : 0, batch : 105, acc 0.613333, loss 0.645892 \n",
      "iter : 0, batch : 106, acc 0.588333, loss 0.654024 \n",
      "iter : 0, batch : 107, acc 0.601667, loss 0.653161 \n",
      "iter : 0, batch : 108, acc 0.633333, loss 0.655226 \n",
      "iter : 0, batch : 109, acc 0.608333, loss 0.648844 \n",
      "iter : 0, batch : 110, acc 0.606667, loss 0.647423 \n",
      "iter : 0, batch : 111, acc 0.583333, loss 0.65045 \n",
      "iter : 0, batch : 112, acc 0.165, loss 0.156821 \n",
      "\n",
      "Turn: 0 -->\n",
      "Train Loss: 0.6520621081886103\n",
      "Test Loss: 0.6671057999875603\n",
      "Train Accuracy: 0.6032754755081738\n",
      "Test Accuracy: 0.5676605504587156\n",
      "\n",
      "iter : 1, batch : 0, acc 0.613333, loss 0.65124 \n",
      "iter : 1, batch : 1, acc 0.601667, loss 0.653877 \n",
      "iter : 1, batch : 2, acc 0.588333, loss 0.654949 \n",
      "iter : 1, batch : 3, acc 0.591667, loss 0.663653 \n",
      "iter : 1, batch : 4, acc 0.61, loss 0.649148 \n",
      "iter : 1, batch : 5, acc 0.631667, loss 0.640132 \n",
      "iter : 1, batch : 6, acc 0.605, loss 0.652473 \n",
      "iter : 1, batch : 7, acc 0.593333, loss 0.6483 \n",
      "iter : 1, batch : 8, acc 0.581667, loss 0.66788 \n",
      "iter : 1, batch : 9, acc 0.611667, loss 0.6517 \n",
      "iter : 1, batch : 10, acc 0.595, loss 0.651052 \n",
      "iter : 1, batch : 11, acc 0.63, loss 0.635444 \n",
      "iter : 1, batch : 12, acc 0.583333, loss 0.659821 \n",
      "iter : 1, batch : 13, acc 0.626667, loss 0.628317 \n",
      "iter : 1, batch : 14, acc 0.58, loss 0.66134 \n",
      "iter : 1, batch : 15, acc 0.608333, loss 0.652052 \n",
      "iter : 1, batch : 16, acc 0.576667, loss 0.661226 \n",
      "iter : 1, batch : 17, acc 0.603333, loss 0.647163 \n",
      "iter : 1, batch : 18, acc 0.648333, loss 0.636085 \n",
      "iter : 1, batch : 19, acc 0.616667, loss 0.642507 \n",
      "iter : 1, batch : 20, acc 0.626667, loss 0.637512 \n",
      "iter : 1, batch : 21, acc 0.6, loss 0.643458 \n",
      "iter : 1, batch : 22, acc 0.638333, loss 0.641784 \n",
      "iter : 1, batch : 23, acc 0.6, loss 0.654463 \n",
      "iter : 1, batch : 24, acc 0.628333, loss 0.641892 \n",
      "iter : 1, batch : 25, acc 0.626667, loss 0.641003 \n",
      "iter : 1, batch : 26, acc 0.606667, loss 0.644214 \n",
      "iter : 1, batch : 27, acc 0.588333, loss 0.66075 \n",
      "iter : 1, batch : 28, acc 0.606667, loss 0.655537 \n",
      "iter : 1, batch : 29, acc 0.625, loss 0.644214 \n",
      "iter : 1, batch : 30, acc 0.595, loss 0.648611 \n",
      "iter : 1, batch : 31, acc 0.626667, loss 0.634874 \n",
      "iter : 1, batch : 32, acc 0.648333, loss 0.623776 \n",
      "iter : 1, batch : 33, acc 0.635, loss 0.639343 \n",
      "iter : 1, batch : 34, acc 0.613333, loss 0.637347 \n",
      "iter : 1, batch : 35, acc 0.638333, loss 0.635701 \n",
      "iter : 1, batch : 36, acc 0.635, loss 0.63104 \n",
      "iter : 1, batch : 37, acc 0.601667, loss 0.651494 \n",
      "iter : 1, batch : 38, acc 0.626667, loss 0.637095 \n",
      "iter : 1, batch : 39, acc 0.615, loss 0.630837 \n",
      "iter : 1, batch : 40, acc 0.6, loss 0.646533 \n",
      "iter : 1, batch : 41, acc 0.618333, loss 0.636599 \n",
      "iter : 1, batch : 42, acc 0.625, loss 0.630037 \n",
      "iter : 1, batch : 43, acc 0.643333, loss 0.625328 \n",
      "iter : 1, batch : 44, acc 0.633333, loss 0.638071 \n",
      "iter : 1, batch : 45, acc 0.68, loss 0.622114 \n",
      "iter : 1, batch : 46, acc 0.611667, loss 0.63702 \n",
      "iter : 1, batch : 47, acc 0.62, loss 0.639311 \n",
      "iter : 1, batch : 48, acc 0.661667, loss 0.624638 \n",
      "iter : 1, batch : 49, acc 0.64, loss 0.636888 \n",
      "iter : 1, batch : 50, acc 0.63, loss 0.630884 \n",
      "iter : 1, batch : 51, acc 0.641667, loss 0.622527 \n",
      "iter : 1, batch : 52, acc 0.636667, loss 0.625747 \n",
      "iter : 1, batch : 53, acc 0.67, loss 0.6145 \n",
      "iter : 1, batch : 54, acc 0.605, loss 0.639654 \n",
      "iter : 1, batch : 55, acc 0.62, loss 0.629944 \n",
      "iter : 1, batch : 56, acc 0.658333, loss 0.617512 \n",
      "iter : 1, batch : 57, acc 0.633333, loss 0.636605 \n",
      "iter : 1, batch : 58, acc 0.636667, loss 0.627107 \n",
      "iter : 1, batch : 59, acc 0.626667, loss 0.633094 \n",
      "iter : 1, batch : 60, acc 0.675, loss 0.606988 \n",
      "iter : 1, batch : 61, acc 0.61, loss 0.639322 \n",
      "iter : 1, batch : 62, acc 0.661667, loss 0.621592 \n",
      "iter : 1, batch : 63, acc 0.646667, loss 0.620345 \n",
      "iter : 1, batch : 64, acc 0.656667, loss 0.619571 \n",
      "iter : 1, batch : 65, acc 0.646667, loss 0.629562 \n",
      "iter : 1, batch : 66, acc 0.665, loss 0.616171 \n",
      "iter : 1, batch : 67, acc 0.613333, loss 0.646885 \n",
      "iter : 1, batch : 68, acc 0.626667, loss 0.632544 \n",
      "iter : 1, batch : 69, acc 0.643333, loss 0.614801 \n",
      "iter : 1, batch : 70, acc 0.635, loss 0.627144 \n",
      "iter : 1, batch : 71, acc 0.648333, loss 0.626561 \n",
      "iter : 1, batch : 72, acc 0.643333, loss 0.623636 \n",
      "iter : 1, batch : 73, acc 0.648333, loss 0.622942 \n",
      "iter : 1, batch : 74, acc 0.661667, loss 0.620631 \n",
      "iter : 1, batch : 75, acc 0.653333, loss 0.62429 \n",
      "iter : 1, batch : 76, acc 0.646667, loss 0.624546 \n",
      "iter : 1, batch : 77, acc 0.666667, loss 0.618681 \n",
      "iter : 1, batch : 78, acc 0.61, loss 0.642479 \n",
      "iter : 1, batch : 79, acc 0.708333, loss 0.611076 \n",
      "iter : 1, batch : 80, acc 0.638333, loss 0.631156 \n",
      "iter : 1, batch : 81, acc 0.643333, loss 0.621938 \n",
      "iter : 1, batch : 82, acc 0.663333, loss 0.619015 \n",
      "iter : 1, batch : 83, acc 0.631667, loss 0.629235 \n",
      "iter : 1, batch : 84, acc 0.653333, loss 0.616681 \n",
      "iter : 1, batch : 85, acc 0.673333, loss 0.627751 \n",
      "iter : 1, batch : 86, acc 0.66, loss 0.613518 \n",
      "iter : 1, batch : 87, acc 0.645, loss 0.631534 \n",
      "iter : 1, batch : 88, acc 0.648333, loss 0.622142 \n",
      "iter : 1, batch : 89, acc 0.641667, loss 0.61999 \n",
      "iter : 1, batch : 90, acc 0.62, loss 0.635548 \n",
      "iter : 1, batch : 91, acc 0.665, loss 0.623902 \n",
      "iter : 1, batch : 92, acc 0.668333, loss 0.613969 \n",
      "iter : 1, batch : 93, acc 0.66, loss 0.614521 \n",
      "iter : 1, batch : 94, acc 0.641667, loss 0.631856 \n",
      "iter : 1, batch : 95, acc 0.621667, loss 0.640703 \n",
      "iter : 1, batch : 96, acc 0.705, loss 0.594641 \n",
      "iter : 1, batch : 97, acc 0.64, loss 0.635065 \n",
      "iter : 1, batch : 98, acc 0.66, loss 0.614771 \n",
      "iter : 1, batch : 99, acc 0.666667, loss 0.616319 \n",
      "iter : 1, batch : 100, acc 0.68, loss 0.607455 \n",
      "iter : 1, batch : 101, acc 0.685, loss 0.605882 \n",
      "iter : 1, batch : 102, acc 0.66, loss 0.613866 \n",
      "iter : 1, batch : 103, acc 0.686667, loss 0.608888 \n",
      "iter : 1, batch : 104, acc 0.635, loss 0.62093 \n",
      "iter : 1, batch : 105, acc 0.665, loss 0.609901 \n",
      "iter : 1, batch : 106, acc 0.66, loss 0.611403 \n",
      "iter : 1, batch : 107, acc 0.661667, loss 0.613791 \n",
      "iter : 1, batch : 108, acc 0.661667, loss 0.623602 \n",
      "iter : 1, batch : 109, acc 0.673333, loss 0.612002 \n",
      "iter : 1, batch : 110, acc 0.665, loss 0.609178 \n",
      "iter : 1, batch : 111, acc 0.686667, loss 0.608109 \n",
      "iter : 1, batch : 112, acc 0.178333, loss 0.150185 \n",
      "\n",
      "Turn: 1 -->\n",
      "Train Loss: 0.6142177074873605\n",
      "Test Loss: 0.6352874909361894\n",
      "Train Accuracy: 0.6614649066801289\n",
      "Test Accuracy: 0.6307339449541285\n",
      "\n",
      "iter : 2, batch : 0, acc 0.661667, loss 0.61203 \n",
      "iter : 2, batch : 1, acc 0.636667, loss 0.624919 \n",
      "iter : 2, batch : 2, acc 0.645, loss 0.62816 \n",
      "iter : 2, batch : 3, acc 0.648333, loss 0.625917 \n",
      "iter : 2, batch : 4, acc 0.7, loss 0.603704 \n",
      "iter : 2, batch : 5, acc 0.691667, loss 0.597618 \n",
      "iter : 2, batch : 6, acc 0.663333, loss 0.613404 \n",
      "iter : 2, batch : 7, acc 0.671667, loss 0.607236 \n",
      "iter : 2, batch : 8, acc 0.653333, loss 0.629508 \n",
      "iter : 2, batch : 9, acc 0.67, loss 0.613658 \n",
      "iter : 2, batch : 10, acc 0.666667, loss 0.617553 \n",
      "iter : 2, batch : 11, acc 0.705, loss 0.597447 \n",
      "iter : 2, batch : 12, acc 0.636667, loss 0.62343 \n",
      "iter : 2, batch : 13, acc 0.656667, loss 0.590691 \n",
      "iter : 2, batch : 14, acc 0.666667, loss 0.626307 \n",
      "iter : 2, batch : 15, acc 0.665, loss 0.613858 \n",
      "iter : 2, batch : 16, acc 0.621667, loss 0.629973 \n",
      "iter : 2, batch : 17, acc 0.651667, loss 0.603955 \n",
      "iter : 2, batch : 18, acc 0.683333, loss 0.611674 \n",
      "iter : 2, batch : 19, acc 0.671667, loss 0.610445 \n",
      "iter : 2, batch : 20, acc 0.671667, loss 0.597233 \n",
      "iter : 2, batch : 21, acc 0.646667, loss 0.611022 \n",
      "iter : 2, batch : 22, acc 0.671667, loss 0.611275 \n",
      "iter : 2, batch : 23, acc 0.65, loss 0.619157 \n",
      "iter : 2, batch : 24, acc 0.663333, loss 0.607894 \n",
      "iter : 2, batch : 25, acc 0.67, loss 0.60706 \n",
      "iter : 2, batch : 26, acc 0.658333, loss 0.612115 \n",
      "iter : 2, batch : 27, acc 0.626667, loss 0.631684 \n",
      "iter : 2, batch : 28, acc 0.661667, loss 0.624091 \n",
      "iter : 2, batch : 29, acc 0.666667, loss 0.617348 \n",
      "iter : 2, batch : 30, acc 0.663333, loss 0.621944 \n",
      "iter : 2, batch : 31, acc 0.66, loss 0.608993 \n",
      "iter : 2, batch : 32, acc 0.696667, loss 0.588123 \n",
      "iter : 2, batch : 33, acc 0.668333, loss 0.612084 \n",
      "iter : 2, batch : 34, acc 0.668333, loss 0.603479 \n",
      "iter : 2, batch : 35, acc 0.695, loss 0.600822 \n",
      "iter : 2, batch : 36, acc 0.683333, loss 0.596104 \n",
      "iter : 2, batch : 37, acc 0.653333, loss 0.619735 \n",
      "iter : 2, batch : 38, acc 0.681667, loss 0.604826 \n",
      "iter : 2, batch : 39, acc 0.686667, loss 0.601311 \n",
      "iter : 2, batch : 40, acc 0.638333, loss 0.615899 \n",
      "iter : 2, batch : 41, acc 0.656667, loss 0.607959 \n",
      "iter : 2, batch : 42, acc 0.67, loss 0.599231 \n",
      "iter : 2, batch : 43, acc 0.69, loss 0.58915 \n",
      "iter : 2, batch : 44, acc 0.676667, loss 0.608319 \n",
      "iter : 2, batch : 45, acc 0.71, loss 0.593658 \n",
      "iter : 2, batch : 46, acc 0.656667, loss 0.609998 \n",
      "iter : 2, batch : 47, acc 0.661667, loss 0.604959 \n",
      "iter : 2, batch : 48, acc 0.701667, loss 0.588416 \n",
      "iter : 2, batch : 49, acc 0.666667, loss 0.6135 \n",
      "iter : 2, batch : 50, acc 0.683333, loss 0.600233 \n",
      "iter : 2, batch : 51, acc 0.691667, loss 0.591186 \n",
      "iter : 2, batch : 52, acc 0.675, loss 0.590501 \n",
      "iter : 2, batch : 53, acc 0.675, loss 0.588331 \n",
      "iter : 2, batch : 54, acc 0.64, loss 0.615153 \n",
      "iter : 2, batch : 55, acc 0.666667, loss 0.604313 \n",
      "iter : 2, batch : 56, acc 0.711667, loss 0.588478 \n",
      "iter : 2, batch : 57, acc 0.676667, loss 0.611461 \n",
      "iter : 2, batch : 58, acc 0.68, loss 0.593104 \n",
      "iter : 2, batch : 59, acc 0.661667, loss 0.608095 \n",
      "iter : 2, batch : 60, acc 0.698333, loss 0.580658 \n",
      "iter : 2, batch : 61, acc 0.656667, loss 0.614913 \n",
      "iter : 2, batch : 62, acc 0.696667, loss 0.596897 \n",
      "iter : 2, batch : 63, acc 0.668333, loss 0.586817 \n",
      "iter : 2, batch : 64, acc 0.676667, loss 0.592428 \n",
      "iter : 2, batch : 65, acc 0.665, loss 0.60639 \n",
      "iter : 2, batch : 66, acc 0.7, loss 0.58056 \n",
      "iter : 2, batch : 67, acc 0.661667, loss 0.621564 \n",
      "iter : 2, batch : 68, acc 0.658333, loss 0.607888 \n",
      "iter : 2, batch : 69, acc 0.68, loss 0.583818 \n",
      "iter : 2, batch : 70, acc 0.68, loss 0.601055 \n",
      "iter : 2, batch : 71, acc 0.673333, loss 0.599902 \n",
      "iter : 2, batch : 72, acc 0.66, loss 0.600859 \n",
      "iter : 2, batch : 73, acc 0.688333, loss 0.591955 \n",
      "iter : 2, batch : 74, acc 0.691667, loss 0.595055 \n",
      "iter : 2, batch : 75, acc 0.685, loss 0.597124 \n",
      "iter : 2, batch : 76, acc 0.671667, loss 0.598481 \n",
      "iter : 2, batch : 77, acc 0.703333, loss 0.596781 \n",
      "iter : 2, batch : 78, acc 0.653333, loss 0.6191 \n",
      "iter : 2, batch : 79, acc 0.713333, loss 0.585575 \n",
      "iter : 2, batch : 80, acc 0.676667, loss 0.602248 \n",
      "iter : 2, batch : 81, acc 0.663333, loss 0.598305 \n",
      "iter : 2, batch : 82, acc 0.693333, loss 0.596018 \n",
      "iter : 2, batch : 83, acc 0.668333, loss 0.602408 \n",
      "iter : 2, batch : 84, acc 0.69, loss 0.58909 \n",
      "iter : 2, batch : 85, acc 0.673333, loss 0.608468 \n",
      "iter : 2, batch : 86, acc 0.671667, loss 0.580891 \n",
      "iter : 2, batch : 87, acc 0.68, loss 0.606038 \n",
      "iter : 2, batch : 88, acc 0.665, loss 0.603539 \n",
      "iter : 2, batch : 89, acc 0.676667, loss 0.593095 \n",
      "iter : 2, batch : 90, acc 0.65, loss 0.61266 \n",
      "iter : 2, batch : 91, acc 0.691667, loss 0.6009 \n",
      "iter : 2, batch : 92, acc 0.693333, loss 0.588116 \n",
      "iter : 2, batch : 93, acc 0.701667, loss 0.591575 \n",
      "iter : 2, batch : 94, acc 0.67, loss 0.602974 \n",
      "iter : 2, batch : 95, acc 0.643333, loss 0.619956 \n",
      "iter : 2, batch : 96, acc 0.735, loss 0.568639 \n",
      "iter : 2, batch : 97, acc 0.671667, loss 0.611417 \n",
      "iter : 2, batch : 98, acc 0.676667, loss 0.58943 \n",
      "iter : 2, batch : 99, acc 0.703333, loss 0.591035 \n",
      "iter : 2, batch : 100, acc 0.708333, loss 0.582443 \n",
      "iter : 2, batch : 101, acc 0.701667, loss 0.582251 \n",
      "iter : 2, batch : 102, acc 0.7, loss 0.582979 \n",
      "iter : 2, batch : 103, acc 0.7, loss 0.582239 \n",
      "iter : 2, batch : 104, acc 0.66, loss 0.600197 \n",
      "iter : 2, batch : 105, acc 0.683333, loss 0.589903 \n",
      "iter : 2, batch : 106, acc 0.691667, loss 0.586758 \n",
      "iter : 2, batch : 107, acc 0.666667, loss 0.591281 \n",
      "iter : 2, batch : 108, acc 0.663333, loss 0.607134 \n",
      "iter : 2, batch : 109, acc 0.691667, loss 0.590024 \n",
      "iter : 2, batch : 110, acc 0.703333, loss 0.585742 \n",
      "iter : 2, batch : 111, acc 0.705, loss 0.584313 \n",
      "iter : 2, batch : 112, acc 0.19, loss 0.145803 \n",
      "\n",
      "Turn: 2 -->\n",
      "Train Loss: 0.591817071661243\n",
      "Test Loss: 0.6148980483833086\n",
      "Train Accuracy: 0.684330873509629\n",
      "Test Accuracy: 0.6708715596330275\n",
      "\n",
      "iter : 3, batch : 0, acc 0.676667, loss 0.589298 \n",
      "iter : 3, batch : 1, acc 0.665, loss 0.6075 \n",
      "iter : 3, batch : 2, acc 0.668333, loss 0.613631 \n",
      "iter : 3, batch : 3, acc 0.671667, loss 0.603244 \n",
      "iter : 3, batch : 4, acc 0.721667, loss 0.576644 \n",
      "iter : 3, batch : 5, acc 0.711667, loss 0.571339 \n",
      "iter : 3, batch : 6, acc 0.685, loss 0.590267 \n",
      "iter : 3, batch : 7, acc 0.711667, loss 0.582402 \n",
      "iter : 3, batch : 8, acc 0.67, loss 0.60641 \n",
      "iter : 3, batch : 9, acc 0.688333, loss 0.590471 \n",
      "iter : 3, batch : 10, acc 0.675, loss 0.598441 \n",
      "iter : 3, batch : 11, acc 0.721667, loss 0.575872 \n",
      "iter : 3, batch : 12, acc 0.661667, loss 0.601811 \n",
      "iter : 3, batch : 13, acc 0.695, loss 0.568179 \n",
      "iter : 3, batch : 14, acc 0.681667, loss 0.603863 \n",
      "iter : 3, batch : 15, acc 0.701667, loss 0.590426 \n",
      "iter : 3, batch : 16, acc 0.648333, loss 0.611195 \n",
      "iter : 3, batch : 17, acc 0.685, loss 0.578553 \n",
      "iter : 3, batch : 18, acc 0.688333, loss 0.597162 \n",
      "iter : 3, batch : 19, acc 0.693333, loss 0.589951 \n",
      "iter : 3, batch : 20, acc 0.69, loss 0.573292 \n",
      "iter : 3, batch : 21, acc 0.678333, loss 0.590911 \n",
      "iter : 3, batch : 22, acc 0.688333, loss 0.592525 \n",
      "iter : 3, batch : 23, acc 0.673333, loss 0.597794 \n",
      "iter : 3, batch : 24, acc 0.685, loss 0.587132 \n",
      "iter : 3, batch : 25, acc 0.681667, loss 0.585862 \n",
      "iter : 3, batch : 26, acc 0.671667, loss 0.59229 \n",
      "iter : 3, batch : 27, acc 0.655, loss 0.613774 \n",
      "iter : 3, batch : 28, acc 0.675, loss 0.606309 \n",
      "iter : 3, batch : 29, acc 0.676667, loss 0.601962 \n",
      "iter : 3, batch : 30, acc 0.675, loss 0.606384 \n",
      "iter : 3, batch : 31, acc 0.68, loss 0.5941 \n",
      "iter : 3, batch : 32, acc 0.723333, loss 0.565829 \n",
      "iter : 3, batch : 33, acc 0.688333, loss 0.595617 \n",
      "iter : 3, batch : 34, acc 0.681667, loss 0.582048 \n",
      "iter : 3, batch : 35, acc 0.71, loss 0.579271 \n",
      "iter : 3, batch : 36, acc 0.693333, loss 0.57486 \n",
      "iter : 3, batch : 37, acc 0.671667, loss 0.600226 \n",
      "iter : 3, batch : 38, acc 0.691667, loss 0.585554 \n",
      "iter : 3, batch : 39, acc 0.703333, loss 0.582738 \n",
      "iter : 3, batch : 40, acc 0.67, loss 0.59589 \n",
      "iter : 3, batch : 41, acc 0.673333, loss 0.589642 \n",
      "iter : 3, batch : 42, acc 0.693333, loss 0.580599 \n",
      "iter : 3, batch : 43, acc 0.713333, loss 0.566019 \n",
      "iter : 3, batch : 44, acc 0.683333, loss 0.589805 \n",
      "iter : 3, batch : 45, acc 0.721667, loss 0.575434 \n",
      "iter : 3, batch : 46, acc 0.675, loss 0.592896 \n",
      "iter : 3, batch : 47, acc 0.691667, loss 0.583077 \n",
      "iter : 3, batch : 48, acc 0.721667, loss 0.566045 \n",
      "iter : 3, batch : 49, acc 0.68, loss 0.598058 \n",
      "iter : 3, batch : 50, acc 0.701667, loss 0.580658 \n",
      "iter : 3, batch : 51, acc 0.713333, loss 0.57116 \n",
      "iter : 3, batch : 52, acc 0.701667, loss 0.568657 \n",
      "iter : 3, batch : 53, acc 0.696667, loss 0.572626 \n",
      "iter : 3, batch : 54, acc 0.651667, loss 0.601188 \n",
      "iter : 3, batch : 55, acc 0.69, loss 0.58728 \n",
      "iter : 3, batch : 56, acc 0.715, loss 0.57048 \n",
      "iter : 3, batch : 57, acc 0.688333, loss 0.595382 \n",
      "iter : 3, batch : 58, acc 0.695, loss 0.571218 \n",
      "iter : 3, batch : 59, acc 0.69, loss 0.592522 \n",
      "iter : 3, batch : 60, acc 0.706667, loss 0.564724 \n",
      "iter : 3, batch : 61, acc 0.666667, loss 0.59948 \n",
      "iter : 3, batch : 62, acc 0.701667, loss 0.580667 \n",
      "iter : 3, batch : 63, acc 0.691667, loss 0.564836 \n",
      "iter : 3, batch : 64, acc 0.691667, loss 0.575364 \n",
      "iter : 3, batch : 65, acc 0.678333, loss 0.592275 \n",
      "iter : 3, batch : 66, acc 0.708333, loss 0.557098 \n",
      "iter : 3, batch : 67, acc 0.665, loss 0.605692 \n",
      "iter : 3, batch : 68, acc 0.673333, loss 0.5924 \n",
      "iter : 3, batch : 69, acc 0.703333, loss 0.563673 \n",
      "iter : 3, batch : 70, acc 0.695, loss 0.583902 \n",
      "iter : 3, batch : 71, acc 0.68, loss 0.58214 \n",
      "iter : 3, batch : 72, acc 0.67, loss 0.58673 \n",
      "iter : 3, batch : 73, acc 0.708333, loss 0.572471 \n",
      "iter : 3, batch : 74, acc 0.7, loss 0.578778 \n",
      "iter : 3, batch : 75, acc 0.688333, loss 0.579378 \n",
      "iter : 3, batch : 76, acc 0.686667, loss 0.582224 \n",
      "iter : 3, batch : 77, acc 0.711667, loss 0.583014 \n",
      "iter : 3, batch : 78, acc 0.673333, loss 0.603049 \n",
      "iter : 3, batch : 79, acc 0.723333, loss 0.568541 \n",
      "iter : 3, batch : 80, acc 0.686667, loss 0.583343 \n",
      "iter : 3, batch : 81, acc 0.675, loss 0.58291 \n",
      "iter : 3, batch : 82, acc 0.688333, loss 0.581266 \n",
      "iter : 3, batch : 83, acc 0.691667, loss 0.584831 \n",
      "iter : 3, batch : 84, acc 0.708333, loss 0.571932 \n",
      "iter : 3, batch : 85, acc 0.678333, loss 0.595132 \n",
      "iter : 3, batch : 86, acc 0.698333, loss 0.559786 \n",
      "iter : 3, batch : 87, acc 0.703333, loss 0.588753 \n",
      "iter : 3, batch : 88, acc 0.663333, loss 0.591784 \n",
      "iter : 3, batch : 89, acc 0.69, loss 0.575308 \n",
      "iter : 3, batch : 90, acc 0.673333, loss 0.596873 \n",
      "iter : 3, batch : 91, acc 0.701667, loss 0.585474 \n",
      "iter : 3, batch : 92, acc 0.708333, loss 0.571982 \n",
      "iter : 3, batch : 93, acc 0.708333, loss 0.576643 \n",
      "iter : 3, batch : 94, acc 0.696667, loss 0.583777 \n",
      "iter : 3, batch : 95, acc 0.666667, loss 0.606673 \n",
      "iter : 3, batch : 96, acc 0.748333, loss 0.551088 \n",
      "iter : 3, batch : 97, acc 0.681667, loss 0.59597 \n",
      "iter : 3, batch : 98, acc 0.701667, loss 0.572632 \n",
      "iter : 3, batch : 99, acc 0.705, loss 0.57461 \n",
      "iter : 3, batch : 100, acc 0.72, loss 0.565258 \n",
      "iter : 3, batch : 101, acc 0.716667, loss 0.565595 \n",
      "iter : 3, batch : 102, acc 0.711667, loss 0.562507 \n",
      "iter : 3, batch : 103, acc 0.705, loss 0.564314 \n",
      "iter : 3, batch : 104, acc 0.665, loss 0.587679 \n",
      "iter : 3, batch : 105, acc 0.683333, loss 0.577147 \n",
      "iter : 3, batch : 106, acc 0.701667, loss 0.57072 \n",
      "iter : 3, batch : 107, acc 0.695, loss 0.576951 \n",
      "iter : 3, batch : 108, acc 0.671667, loss 0.597518 \n",
      "iter : 3, batch : 109, acc 0.706667, loss 0.57524 \n",
      "iter : 3, batch : 110, acc 0.706667, loss 0.569854 \n",
      "iter : 3, batch : 111, acc 0.723333, loss 0.569613 \n",
      "iter : 3, batch : 112, acc 0.183333, loss 0.142573 \n",
      "\n",
      "Turn: 3 -->\n",
      "Train Loss: 0.5770525986048437\n",
      "Test Loss: 0.6005158746020787\n",
      "Train Accuracy: 0.6966398907184962\n",
      "Test Accuracy: 0.6869266055045872\n",
      "\n",
      "iter : 4, batch : 0, acc 0.71, loss 0.574713 \n",
      "iter : 4, batch : 1, acc 0.668333, loss 0.595567 \n",
      "iter : 4, batch : 2, acc 0.668333, loss 0.604758 \n",
      "iter : 4, batch : 3, acc 0.685, loss 0.588012 \n",
      "iter : 4, batch : 4, acc 0.733333, loss 0.55887 \n",
      "iter : 4, batch : 5, acc 0.735, loss 0.553662 \n",
      "iter : 4, batch : 6, acc 0.701667, loss 0.575459 \n",
      "iter : 4, batch : 7, acc 0.723333, loss 0.565663 \n",
      "iter : 4, batch : 8, acc 0.676667, loss 0.591003 \n",
      "iter : 4, batch : 9, acc 0.7, loss 0.574639 \n",
      "iter : 4, batch : 10, acc 0.686667, loss 0.586211 \n",
      "iter : 4, batch : 11, acc 0.716667, loss 0.562835 \n",
      "iter : 4, batch : 12, acc 0.688333, loss 0.587778 \n",
      "iter : 4, batch : 13, acc 0.723333, loss 0.55338 \n",
      "iter : 4, batch : 14, acc 0.685, loss 0.588018 \n",
      "iter : 4, batch : 15, acc 0.716667, loss 0.574926 \n",
      "iter : 4, batch : 16, acc 0.661667, loss 0.598572 \n",
      "iter : 4, batch : 17, acc 0.696667, loss 0.562279 \n",
      "iter : 4, batch : 18, acc 0.7, loss 0.587468 \n",
      "iter : 4, batch : 19, acc 0.703333, loss 0.575575 \n",
      "iter : 4, batch : 20, acc 0.703333, loss 0.557796 \n",
      "iter : 4, batch : 21, acc 0.696667, loss 0.577191 \n",
      "iter : 4, batch : 22, acc 0.698333, loss 0.580089 \n",
      "iter : 4, batch : 23, acc 0.69, loss 0.583761 \n",
      "iter : 4, batch : 24, acc 0.698333, loss 0.57318 \n",
      "iter : 4, batch : 25, acc 0.696667, loss 0.571477 \n",
      "iter : 4, batch : 26, acc 0.696667, loss 0.57872 \n",
      "iter : 4, batch : 27, acc 0.676667, loss 0.60155 \n",
      "iter : 4, batch : 28, acc 0.686667, loss 0.595337 \n",
      "iter : 4, batch : 29, acc 0.686667, loss 0.592425 \n",
      "iter : 4, batch : 30, acc 0.703333, loss 0.596294 \n",
      "iter : 4, batch : 31, acc 0.693333, loss 0.58479 \n",
      "iter : 4, batch : 32, acc 0.713333, loss 0.5509 \n",
      "iter : 4, batch : 33, acc 0.693333, loss 0.584827 \n",
      "iter : 4, batch : 34, acc 0.69, loss 0.567465 \n",
      "iter : 4, batch : 35, acc 0.718333, loss 0.56485 \n",
      "iter : 4, batch : 36, acc 0.715, loss 0.560976 \n",
      "iter : 4, batch : 37, acc 0.675, loss 0.587245 \n",
      "iter : 4, batch : 38, acc 0.705, loss 0.572867 \n",
      "iter : 4, batch : 39, acc 0.718333, loss 0.57017 \n",
      "iter : 4, batch : 40, acc 0.688333, loss 0.581882 \n",
      "iter : 4, batch : 41, acc 0.678333, loss 0.576787 \n",
      "iter : 4, batch : 42, acc 0.705, loss 0.568673 \n",
      "iter : 4, batch : 43, acc 0.726667, loss 0.550271 \n",
      "iter : 4, batch : 44, acc 0.693333, loss 0.577403 \n",
      "iter : 4, batch : 45, acc 0.721667, loss 0.562892 \n",
      "iter : 4, batch : 46, acc 0.69, loss 0.581247 \n",
      "iter : 4, batch : 47, acc 0.703333, loss 0.568146 \n",
      "iter : 4, batch : 48, acc 0.718333, loss 0.551278 \n",
      "iter : 4, batch : 49, acc 0.691667, loss 0.586981 \n",
      "iter : 4, batch : 50, acc 0.708333, loss 0.567342 \n",
      "iter : 4, batch : 51, acc 0.72, loss 0.557523 \n",
      "iter : 4, batch : 52, acc 0.718333, loss 0.554149 \n",
      "iter : 4, batch : 53, acc 0.705, loss 0.562482 \n",
      "iter : 4, batch : 54, acc 0.663333, loss 0.592584 \n",
      "iter : 4, batch : 55, acc 0.708333, loss 0.575248 \n",
      "iter : 4, batch : 56, acc 0.728333, loss 0.558593 \n",
      "iter : 4, batch : 57, acc 0.695, loss 0.584241 \n",
      "iter : 4, batch : 58, acc 0.7, loss 0.55624 "
     ]
    }
   ],
   "source": [
    "# fit\n",
    "convNet = SimpleConvNet()\n",
    "convNet.fit(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
