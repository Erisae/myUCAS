{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# 交叉熵\n",
    "def cross_entropy(y, y_hat, D):\n",
    "    return - np.sum(D*y*np.log(y_hat+1e-16))\n",
    "\n",
    "class Linear_classifier:\n",
    "    \n",
    "    def __init__(self,w,b,batch_size):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def softmax(x):\n",
    "        x_exp = np.exp(x-np.max(x,axis = 0))\n",
    "        total = np.sum(x_exp,axis=0) # 列\n",
    "        return x_exp / total\n",
    "        \n",
    "    # 线性模型\n",
    "    def linear_model(self,x):\n",
    "        return Linear_classifier.softmax(np.dot(self.w.T,x)+self.b)\n",
    "\n",
    "    # 梯度下降\n",
    "    def sgd(self, w, b, x, y, D, alpha=0.01):\n",
    "        batch_size = x.shape[1]\n",
    "        lm = Linear_classifier.linear_model(self,x)\n",
    "        out = D*(y - lm) # 带权重的softmax求导\n",
    "        dw = np.dot(out,x.T)\n",
    "        db = np.sum(out, axis = 1,keepdims = True) # aaa\n",
    "        w_o = w + alpha*dw.T/batch_size\n",
    "        b_o = b + alpha*db/batch_size\n",
    "        return w_o,b_o\n",
    "\n",
    "    def get_minibatch(self, epoch, x, y, D):\n",
    "        rounds = 67349 // self.batch_size\n",
    "        index1 = (epoch % rounds) * self.batch_size\n",
    "        index2 = (epoch % rounds + 1) * self.batch_size\n",
    "        return x[:,int(index1) :int(index2)], y[:,int(index1):int(index2)], D[:,int(index1):int(index2)]\n",
    "\n",
    "    def train(self, x, y, D, epoches = 2000, learning_rate = 0.01):\n",
    "        for epoch in range(epoches):\n",
    "            # 获得用于训练的minibatch\n",
    "            minibatch_x,minibatch_y,minibatch_D = Linear_classifier.get_minibatch(self, epoch, x, y, D)\n",
    "            # 预测，计算loss和acc\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                y_pred = Linear_classifier.linear_model(self, minibatch_x)\n",
    "                loss = cross_entropy(minibatch_y, y_pred, minibatch_D)\n",
    "                acc = (np.argmax(y_pred,axis = 0) == np.argmax(minibatch_y, axis = 0)).sum() / minibatch_x.shape[1]\n",
    "                print(\"after %d terms, the train loss is %g, the train acc on training set is %g\"%(epoch, loss, acc))\n",
    "            \n",
    "            # 梯度下降\n",
    "            self.w,self.b = Linear_classifier.sgd(self, self.w, self.b, minibatch_x, minibatch_y, minibatch_D)\n",
    "            \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_linear_classifer(x, y, D, batch_size, epoches, learning_rate):\n",
    "    # 初始化wb参数\n",
    "    w_dat = np.random.randint(1,100,size=(784,10))\n",
    "    w = w_dat / w_dat.sum(axis=0)\n",
    "    b = np.zeros((10,1))\n",
    "\n",
    "    # 建立模型\n",
    "    model = Linear_classifier(w, b, batch_size)\n",
    "\n",
    "    # 训练模型\n",
    "    model.train(x,y,D,epoches,learning_rate)\n",
    "\n",
    "    # 测试（训练集）\n",
    "    y_pred = model.linear_model(x)\n",
    "\n",
    "    # 获得one-hot形式的y_pred，y_low\n",
    "    classEst = np.zeros((y_pred.shape))\n",
    "    classEst[np.argmax(y_pred,axis=0), range(y_pred.shape[1])] = 1\n",
    "    classEst_low = np.argmax(classEst,axis = 0).reshape(1,y_pred.shape[1])\n",
    "    y_low = np.argmax(y,axis = 0).reshape(1,y_pred.shape[1])\n",
    "\n",
    "    # 计算error\n",
    "    error = ((classEst_low != y_low)*D[0].reshape(1,y_pred.shape[1])).sum() # 保证了error<1恒成立\n",
    "\n",
    "    # 返回结果\n",
    "    return model,error,classEst,classEst_low\n",
    "\n",
    "\n",
    "# adaboost\n",
    "def adaBoostTrainDS(x, y, numIt=20, learning_rate = 0.1):# aaa\n",
    "\n",
    "    '''\n",
    "    D : (1,m) m:样本个数\n",
    "    aggClassEst：（1,m）每个样本分到的类\n",
    "    '''\n",
    "    weakClassArr = []                 # 保存弱分类器数组\n",
    "    alphaArr = []\n",
    "    DArr = []\n",
    "    m = x.shape[1]\n",
    "    D = np.ones((1, m)) / m           # D向量 每条样本所对应的一个权重\n",
    "    aggClassEst = np.zeros((10, m))    # 统计类别估计累积值\n",
    "    batch_size = 60000 # aaa\n",
    "    print(y.shape)\n",
    "    y_low = np.argmax(y, axis=0).reshape(1,m)\n",
    "    train_loss = []\n",
    "    train_acc  = []\n",
    "\n",
    "    for i in range(numIt):\n",
    "        # D1是一维D的按行扩展\n",
    "        D1 = np.repeat(D.reshape(1,m),10,axis=0) \n",
    "        print(\".....weak classifier %d is generating.....\"%i)\n",
    "\n",
    "        linear_model, error, classEst, classEst_low = add_linear_classifer(x, y, D1, batch_size, 1000, 0.01)\n",
    "\n",
    "        # 计算float\n",
    "        alpha = float(learning_rate * (np.log((1.0 - error) / max(error, 1e-16)) + np.log(9)))\n",
    "\n",
    "        # 记录弱分类器（参数）和alpha\n",
    "        alphaArr.append(alpha)\n",
    "        weakClassArr.append(linear_model)  \n",
    "\n",
    "        # 计算e\n",
    "        expon = alpha*(classEst_low != y_low)\n",
    "\n",
    "        # 计算D\n",
    "        D = np.multiply(D, np.exp(expon)) \n",
    "        D = D / D.sum() \n",
    "        DZ = np.ones((D1.shape))\n",
    "\n",
    "        # 累加结果\n",
    "        aggClassEst += alpha * classEst #error小则alpha大，对结果的占比高\n",
    "\n",
    "        # acc和loss\n",
    "        acc = (np.argmax(aggClassEst, axis = 0) == np.argmax(y, axis = 0)).sum() / m\n",
    "        loss = cross_entropy(y, aggClassEst, DZ) / m\n",
    "        train_acc.append(acc)\n",
    "        train_loss.append(loss)\n",
    "        print(\"...after generating weak classifier %d, the acc is %g, loss is %g.\"%(i, acc, loss))\n",
    "        \n",
    "    return weakClassArr, aggClassEst, alphaArr, DArr, train_loss, train_acc\n",
    "\n",
    "def Test_adaBoost(x, y, weakClassArr, alphaArr, DArr, test_train = False):\n",
    "    aggClassEst = np.zeros((y.shape[0],x.shape[1]))\n",
    "    index = 0\n",
    "    DZ = np.ones((y.shape[0],x.shape[1]))\n",
    "    test_acc  = []\n",
    "    test_loss = []\n",
    "    for classifier in weakClassArr:\n",
    "        y_pred = classifier.linear_model(x)\n",
    "        classest = np.zeros((y_pred.shape))\n",
    "        classest[np.argmax(y_pred,axis=0),range(y_pred.shape[1])] = 1\n",
    "        aggClassEst += classest * alphaArr[index]\n",
    "        index += 1\n",
    "        acc = (np.argmax(aggClassEst,axis=0) == np.argmax(y,axis=0)).sum() / x.shape[1]\n",
    "        loss = cross_entropy(y, aggClassEst, DZ) / x.shape[1]\n",
    "        test_acc.append(acc)\n",
    "        test_loss.append(loss)\n",
    "    if test_train:\n",
    "        setname = \"TRAINING\"\n",
    "    else:\n",
    "        setname = \"TESTING\"\n",
    "\n",
    "    print(\"accuracy on \" + setname + \" set is : \"+ str(acc))\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 60000)\n",
      ".....weak classifier 0 is generating.....\n",
      "after 0 terms, the train loss is 3.27235, the train acc on training set is 0.117317\n",
      "after 100 terms, the train loss is 1.98195, the train acc on training set is 0.317783\n",
      "after 200 terms, the train loss is 1.49666, the train acc on training set is 0.5009\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据\n",
    "x_train = np.load(\"x_train_number.npy\",allow_pickle=True) \n",
    "y_train = np.load(\"y_train_number.npy\",allow_pickle=True) \n",
    "x_test  = np.load(\"x_test_number.npy\",allow_pickle=True)\n",
    "y_test  = np.load(\"y_test_number.npy\",allow_pickle=True)\n",
    "x_train = x_train.T.astype(\"float\")\n",
    "x_test  = x_test.T.astype(\"float\")\n",
    "\n",
    "# 训练：\n",
    "weakClassArr, aggClassEst, alphaArr, DArr, train_loss, train_acc = adaBoostTrainDS(x_train,y_train)\n",
    "Test_adaBoost(x_train,y_train, weakClassArr, alphaArr, DArr, test_train = True)\n",
    "# 测试\n",
    "test_loss, test_acc = Test_adaBoost(x_test, y_test, weakClassArr, alphaArr, DArr)\n",
    "\n",
    "# 画出损失函数图\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss,color='b')\n",
    "plt.plot(test_loss,color='r')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig('ada_loss_mnist.png')\n",
    "\n",
    "# acc图\n",
    "plt.figure(2)\n",
    "plt.plot(train_acc,color='b')\n",
    "plt.plot(test_acc,color='r')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.savefig('ada_acc_mnist.png')\n"
   ]
  }
 ]
}