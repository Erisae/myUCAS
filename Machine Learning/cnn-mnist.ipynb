{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ,os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "x_train = np.load(\"x_train_number.npy\") \n",
    "y_train = np.load(\"y_train_number.npy\")\n",
    "x_test  = np.load(\"x_test_number.npy\")\n",
    "y_test  = np.load(\"y_test_number.npy\")\n",
    "x_train = x_train.reshape(60000,1,28,28)/255.0\n",
    "y_train = y_train.T\n",
    "x_test = x_test.reshape(10000,1,28,28)/255.0\n",
    "y_test = y_test.T\n",
    "\n",
    "import sys ,os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import math\n",
    "# 激活函数\n",
    "def relu(x):\n",
    "    x_1 = np.where(x < 0 ,0,x)\n",
    "    return x_1\n",
    "\n",
    "def softmax(x):\n",
    "    exp_a = np.exp(x-np.max(x,axis = 1,keepdims=True)) # 行\n",
    "    sum_exp_a = np.sum(exp_a,axis = 1,keepdims=True)\n",
    "    ret = exp_a/sum_exp_a\n",
    "    return ret\n",
    "\n",
    "def cross_entropy(y,y_hat):\n",
    "    return -np.sum(y*np.log(y_hat + np.exp(-7)))\n",
    "\n",
    "def get_im2col_indices(x_shape, field_height, field_width, stride=1,padding=0):\n",
    "  # First figure out what the size of the output should be\n",
    "  N, C, H, W = x_shape\n",
    "  assert (H + 2 * padding - field_height) % stride == 0\n",
    "  assert (W + 2 * padding - field_height) % stride == 0\n",
    "  out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "  out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "  i0 = np.repeat(np.arange(field_height), field_width)\n",
    "  i0 = np.tile(i0, C)\n",
    "  i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "  j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "  j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "  i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "  j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "  k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "  return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col(x, field_height, field_width, stride=1 ,padding=0):\n",
    "  \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "  # Zero-pad the input\n",
    "  p = padding\n",
    "  x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "  k, i, j = get_im2col_indices(x.shape, field_height, field_width, stride, padding)\n",
    "\n",
    "  cols = x_padded[:, k, i, j]\n",
    "  C = x.shape[1]\n",
    "  cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "  return cols\n",
    "\n",
    "\n",
    "def col2im(cols, x_shape, field_height, field_width, stride=1, padding=0):\n",
    "  \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "  N, C, H, W = x_shape\n",
    "  H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "  x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "  k, i, j = get_im2col_indices(x_shape, field_height, field_width, stride, padding)\n",
    "  cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "  cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "  np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "  if padding == 0:\n",
    "    return x_padded\n",
    "  return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "pass\n",
    "\n",
    "\n",
    "# 卷积层\n",
    "class Convolution:\n",
    "    def __init__(self, w, bias, stride = 1, pad = 0):\n",
    "        self.x = None\n",
    "        self.w = w\n",
    "        self.b  = bias\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def conv(self, x, w, bias, stride, pad):\n",
    "        #获取输入x、卷积核的维度\n",
    "        FN,NC,FH,FW = w.shape\n",
    "        m,nc_in, h_in,w_in = x.shape\n",
    "    \n",
    "        #计算输出的height和width\n",
    "        h_out = int((h_in+2*pad-FH)/stride + 1)\n",
    "        w_out = int((w_in+2*pad-FW)/stride + 1)\n",
    "\n",
    "        #将x按照行展开（m*h_out*w_out,FH*FW*NC)\n",
    "        #此时不同的sample同一个channel的值是相邻的\n",
    "        x_col = im2col(x,FH,FW,stride,pad).T\n",
    "        \n",
    "        #将卷积核按列展开(FH*FW*C,FN)\n",
    "        w_col = w.reshape(FN,FH*FW*NC).T\n",
    "\n",
    "        #计算卷积 (m*h_out*w_out,FN)->T->reshape\n",
    "        out = (np.dot(x_col,w_col)+bias)\n",
    "       \n",
    "        #将batch提到最高维度（共三维）\n",
    "        out_1 = np.zeros((m,h_out*w_out,FN))\n",
    "        for i in range(m*h_out*w_out):\n",
    "            out_1[i%m,i//m,:] = out[i,:]\n",
    "        #最后调整维度\n",
    "        out = out_1.transpose((0,2,1)).reshape(m,FN,h_out,w_out)\n",
    "        return out,x_col\n",
    "    \n",
    "    def central_symmetry(self,w):\n",
    "        w_r=w.transpose((2,3,0,1))\n",
    "        w_r = np.flipud(np.fliplr(w_r))\n",
    "        w_r=w_r.transpose((3,2,0,1))# 2,3,0,1 交换前两个，因为这时候的卷积核个数变了\n",
    "        return w_r\n",
    "\n",
    "        \n",
    "    def forward (self,x):\n",
    "        self.x = x # x并不固定，不在初始化的时候直接赋值\n",
    "        out,x_col = Convolution.conv(self,x,self.w,self.b,self.stride,self.pad)\n",
    "        self.out_shape = out.shape\n",
    "        self.x_col = x_col\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout, learning_rate = 0.01):\n",
    "        # shape为：（h_out*w_out*m，nc_out）不同batch的相邻的channel相邻\n",
    "        dout_col = dout.transpose(1,0,2,3).reshape(self.w.shape[0],-1).T\n",
    "\n",
    "        #dx dx是正确的，但是dx应该在w更新之前更新\n",
    "        w_r = Convolution.central_symmetry(self,self.w)\n",
    "        bias_z = np.zeros((1,self.w.shape[1]))\n",
    "        dx,_ = Convolution.conv(self,dout,w_r,bias_z,1,w_r.shape[3]- 1) # padding\n",
    "\n",
    "        dw = dout.transpose(1, 2, 3, 0).reshape(self.w.shape[0], -1).dot(self.x_col).reshape(self.w.shape)\n",
    "        self.w = self.w - learning_rate*dw /float(dout.shape[0])\n",
    "\n",
    "        db = np.sum(dout_col,axis=0,keepdims=True).reshape(1,-1)\n",
    "        self.b = self.b - learning_rate*db /float(dout.shape[0])\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "# 池化层\n",
    "class Pooling:\n",
    "    def __init__(self,pool_h,pool_w,stride = 1,pad = 0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        self.x_col = None\n",
    "        \n",
    "    def forward ( self,x) :\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        h_out = int(1+(H-self.pool_h)/self.stride)\n",
    "        w_out = int(1+(W-self.pool_w)/self.stride)\n",
    "\n",
    "        # 展开 (N*h_out*w_out,H*W*C)\n",
    "        x_col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "\n",
    "        # 以N为最高维reshape\n",
    "        x_col_r = np.zeros((N,h_out*w_out,self.pool_h*self.pool_w*C))\n",
    "        for i in range(N*h_out*w_out):\n",
    "            x_col_r[i%N,i//N,:]=x_col[:,i] # 用的是x_col的转置\n",
    "        x_col = x_col_r.transpose(0,2,1)\n",
    "\n",
    "        # 以channel为最高维reshape\n",
    "        x_col = x_col.reshape(N,C,self.pool_w*self.pool_h,h_out*w_out).transpose(0,1,3,2)\n",
    "        self.x_col = x_col \n",
    "        # 最大值\n",
    "        arg_max = np.argmax(x_col,axis=3)\n",
    "        self.arg_max = arg_max \n",
    "        out = np.max(x_col,axis=3)\n",
    "        out =out.reshape(N,C,h_out,w_out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self ,dout):\n",
    " \n",
    "        N, C, H, W = self.x.shape\n",
    "        # 根据arg_max生成二维damx\n",
    "        pool_size = self.pool_h*self.pool_w\n",
    "        dmax = np.zeros((dout.size,pool_size)) \n",
    "        dmax[np.arange(self.arg_max.size),self.arg_max.flatten()] = dout.flatten()\n",
    "\n",
    "        # dmax矩阵中，不同batch分开存放，为使用col2im将其变为在一起（类似im2col结果）\n",
    "        dmax_1 = np.zeros((dmax.shape))\n",
    "        size = dmax.shape[0] // N\n",
    "        for i in range(dmax.shape[0]):\n",
    "            dmax_1[(i%size) * N + i//size,:] = dmax[i,:]\n",
    "        dmax = dmax_1\n",
    "\n",
    "        # 为和im2col的情况一样，将二维d_max中按列连接的不同channel数据分开放在不同列中\n",
    "        dmax_2 = np.zeros((dmax.shape[0] //C , dmax.shape[1] *C))\n",
    "        for i in range(C):\n",
    "            dmax_2[:,i*dmax.shape[1]:(i+1)*dmax.shape[1]] = dmax[i*dmax_2.shape[0]:(i+1)*dmax_2.shape[0],:]\n",
    "        dmax = dmax_2.T # 注意要转置\n",
    "        \n",
    "        # 获得dx\n",
    "        dx = col2im(dmax,self.x.shape,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        return dx\n",
    "\n",
    "\n",
    "# relu层\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self ,x):\n",
    "        self.mask = x <= 0\n",
    "        out = x\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout \n",
    "        return dx\n",
    "\n",
    "\n",
    "# softmax层\n",
    "class SoftMax:\n",
    "    def __init__ (self):\n",
    "        self.y_hat = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        self.y_hat = softmax(x)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def backward(self,labels):\n",
    "        m = labels.shape[0]\n",
    "        dx = self.y_hat.copy()\n",
    "        dx = dx - labels\n",
    "        return dx\n",
    "\n",
    "\n",
    "# affine fc层\n",
    "class Affine:\n",
    "    def __init__(self,w,b):\n",
    "        self.w = w # shape is (n_x,n_unit)\n",
    "        self.b  = b  # shape is(1,n_unit)\n",
    "        self.X = None\n",
    "        self.origin_x_shape = None\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "        self.out_shape =None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.origin_x_shape = x.shape \n",
    "        # 将四维的xreshape成为2维的，batch在第一维\n",
    "        self.x = x.reshape(x.shape[0],-1)#(m,n)\n",
    "        out =  np.dot(self.x, self.w)+self.b\n",
    "        self.out_shape = out.shape\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout,learning_rate = 0.01):\n",
    "        m = float(self.x.shape[0])\n",
    "        \n",
    "        self.dw = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0,keepdims=True)\n",
    " \n",
    "        dx = np.dot(dout,self.w.T)\n",
    "        \n",
    "        dx = dx.reshape(self.origin_x_shape) # 保持与之前的x一样的shape\n",
    "        \n",
    "        #更新W和b\n",
    "        self.w = self.w-learning_rate*self.dw/m\n",
    "        self.b = self.b - learning_rate*self.db/m\n",
    "        return dx\n",
    "\n",
    "# model\n",
    "class SimpleConvNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.Y= None\n",
    "        self.layers = []\n",
    "\n",
    "    def add_conv_layer(self, core_num, cha_num, f, stride=1, pad=0):\n",
    "        # 初始化W，b\n",
    "        w = np.random.randn(core_num, cha_num, f, f)*0.1\n",
    "        bias = np.zeros((1, core_num))\n",
    "        # 卷积层\n",
    "        Conv = Convolution(w, bias, stride=stride, pad=pad)\n",
    "        return Conv\n",
    "\n",
    "    def add_maxpool_layer(self, pool_shape, stride=1, pad=0):\n",
    "        pool_h, pool_w = pool_shape\n",
    "        pool = Pooling(pool_h, pool_w, stride=stride, pad=pad)\n",
    "        \n",
    "        return pool\n",
    "    \n",
    "    def add_affine(self, m, n_units):\n",
    "        w= np.random.randn(m, n_units)*0.1\n",
    "        \n",
    "        b = np.zeros((1, n_units))\n",
    "        \n",
    "        fc_layer = Affine(w,b)\n",
    "        \n",
    "        return fc_layer\n",
    "    \n",
    "    def add_relu(self):\n",
    "        relu_layer =  Relu()\n",
    "        return relu_layer\n",
    "    \n",
    "    \n",
    "    def add_softmax(self):\n",
    "        softmax_layer = SoftMax()\n",
    "        return softmax_layer\n",
    "    \n",
    "    #计算卷积或池化后的H和W\n",
    "    def cacl_out_hw(self,HW,f,stride = 1,pad = 0): # 卷积核的大小\n",
    "        return (HW+2*pad - f)/stride+1\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def init_model(self,train_X,n_classes,continue_train = False):\n",
    "        save_index = []\n",
    "        N,C,H,W = train_X.shape\n",
    "\n",
    "        #卷积层 卷积核大小为5*5，步长是1，一共有6个卷积核\n",
    "        n_filter = 6\n",
    "        f = 5\n",
    "        conv_layer = self.add_conv_layer(n_filter,C,f,stride=1)\n",
    "        out_h = self.cacl_out_hw(H,f)\n",
    "        out_w = self.cacl_out_hw(W,f)\n",
    "        out_ch = n_filter\n",
    "        self.layers.append(conv_layer) # layer记录层是什么\n",
    "        save_index.append(0)\n",
    "\n",
    "        #池化 池化核大小为2*2，步长是2，最大池化\n",
    "        f = 2\n",
    "        pool_layer = self.add_maxpool_layer(pool_shape=(f,f),stride=2) #aaa stride =2 原来\n",
    "        out_h = self.cacl_out_hw(out_h,f,stride=2)\n",
    "        out_w = self.cacl_out_hw(out_w,f,stride=2)\n",
    "        self.layers.append(pool_layer)\n",
    "        \n",
    "        #卷积层2 卷积核大小为5*5，步长1，一共有16个卷积核\n",
    "        n_filter = 16\n",
    "        f = 5\n",
    "        conv_layer2 = self.add_conv_layer(n_filter,out_ch,f,stride=1)\n",
    "        out_h = self.cacl_out_hw(out_h,f)\n",
    "        out_w = self.cacl_out_hw(out_w,f)\n",
    "        out_ch = n_filter\n",
    "        self.layers.append(conv_layer2) # layer记录层是什么\n",
    "        save_index.append(2)\n",
    "        \n",
    "        # relu\n",
    "        relu_layer = self.add_relu()\n",
    "        self.layers.append(relu_layer) \n",
    "        \n",
    "\n",
    "        #池化2 池化核大小为2*2，步长是2，最大池化\n",
    "        f = 2\n",
    "        pool_layer2 = self.add_maxpool_layer(pool_shape=(f,f),stride=2) #aaa stride =2 原来\n",
    "        out_h = self.cacl_out_hw(out_h,f,stride=2)\n",
    "        out_w = self.cacl_out_hw(out_w,f,stride=2)\n",
    "        self.layers.append(pool_layer2)\n",
    "\n",
    "        #FC\n",
    "        n_units=int(out_h*out_w*out_ch)\n",
    "        fc_layer = self.add_affine(n_units,n_classes)\n",
    "        self.layers.append(fc_layer)\n",
    "        save_index.append(5)\n",
    "\n",
    "        #SoftMax\n",
    "        softmax_layer = self.add_softmax()\n",
    "        self.layers.append(softmax_layer)\n",
    "        self.save_index = save_index\n",
    "        if continue_train :\n",
    "            save=np.load(\"D:\\\\torch_work\\\\cnn_param.npy\",allow_pickle= True)\n",
    "            i = 0\n",
    "            for index in save_index:\n",
    "                self.layers[index].w = save[i]\n",
    "                i = i+1\n",
    "                self.layers[index].b = save[i]\n",
    "                i = i+1\n",
    "        \n",
    "    def forward_progation(self,train_X, print_out = False): # 优化 用一个循环就可以了 \n",
    "        N,C,H,W = train_X.shape\n",
    "        index = 0\n",
    "        X = train_X\n",
    "        for index in range(len(self.layers)):\n",
    "            layer = self.layers[index]\n",
    "            X = layer.forward(X)\n",
    "            if print_out:\n",
    "                print(str(index)+\"次操作之后：\"+str(X.shape))\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    def back_progation(self,train_y,learning_rate = 0.01):\n",
    "        save =[]\n",
    "        dout = train_y\n",
    "        for index in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[index]\n",
    "            dout = layer.backward(dout)\n",
    "        for index in self.save_index:\n",
    "            save.append(self.layers[index].w)\n",
    "            save.append(self.layers[index].b)\n",
    "        save = np.array(save)\n",
    "        np.save(\"D:\\\\torch_work\\\\cnn_param\",save)\n",
    "        \n",
    "      \n",
    "    def get_minibatch(self,batch_data,minibatch_size,num):\n",
    "        m_examples = batch_data.shape[0]\n",
    "        minibatches = math.ceil( m_examples / minibatch_size) # “向上取整”\n",
    " \n",
    "        if(num < minibatches):\n",
    "            return batch_data[num*minibatch_size:(num+1)*minibatch_size]\n",
    "        else:\n",
    "            return batch_data[num*minibatch_size:m_examples]\n",
    "    \n",
    "    \n",
    "    def optimize(self,train_x, train_y,test_x, test_y, minibatch_size,learning_rate=0.01,num_iters=500):\n",
    "\n",
    "        m = train_x.shape[0]\n",
    "        num_batches  = math.ceil(m / minibatch_size)\n",
    "\n",
    "        train_losss = [] # 训练集上的loss\n",
    "        test_losss  = []\n",
    "        train_accs  = [] # 训练集上的acc\n",
    "        test_accs   = []\n",
    "        \n",
    "        for iteration in range(num_iters):\n",
    "            for batch_num in range(num_batches):\n",
    "                minibatch_x = self.get_minibatch(train_x,minibatch_size,batch_num)\n",
    "                minibatch_y = self.get_minibatch(train_y,minibatch_size,batch_num)\n",
    "                # 前向传播\n",
    "                A = self.forward_progation(minibatch_x)\n",
    "                # 反向传播\n",
    "                self.back_progation(minibatch_y,learning_rate)\n",
    "                acc = (np.argmax(A,axis = 1) == np.argmax(minibatch_y, axis = 1)).sum() / minibatch_size\n",
    "                loss = cross_entropy(minibatch_y, A) / minibatch_size\n",
    "                print(\"iter : %d, batch : %d, acc %g, loss %g \"%(iteration, batch_num, acc, loss))\n",
    "            y_pred     = self.forward_progation(train_x)\n",
    "            y_pred_t   = self.forward_progation(test_x)\n",
    "            train_acc  = (np.argmax(y_pred,axis = 1) == np.argmax(train_y, axis = 1)).sum() / train_x.shape[0]\n",
    "            test_acc   = (np.argmax(y_pred_t,axis = 1) == np.argmax(test_y, axis = 1)).sum() / test_x.shape[0]\n",
    "            train_loss = cross_entropy(train_y, y_pred) / train_x.shape[0]\n",
    "            test_loss  = cross_entropy(test_y, y_pred_t) / test_x.shape[0]\n",
    "            print(f\"\\nTurn: {iteration} -->\\n\"\n",
    "              f\"Train Loss: {train_loss}\\n\"\n",
    "              f\"Test Loss: {test_loss}\\n\"\n",
    "              f\"Train Accuracy: {train_acc}\\n\"\n",
    "              f\"Test Accuracy: {test_acc}\\n\")\n",
    "            train_losss.append(train_loss)\n",
    "            test_accs.append(test_acc)\n",
    "            train_accs.append(train_acc)\n",
    "            test_losss.append(test_loss)\n",
    "            \n",
    "        #画出损失函数图\n",
    "        plt.figure(1)\n",
    "        plt.plot(train_losss,color='b')\n",
    "        plt.plot(test_losss,color='r')\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.savefig('cnn_loss_r.png')\n",
    "\n",
    "        #acc图\n",
    "        plt.figure(2)\n",
    "        plt.plot(train_accs,color='b')\n",
    "        plt.plot(test_accs,color='r')\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"acc\")\n",
    "        plt.savefig('cnn_acc_r.png')\n",
    "        \n",
    "       \n",
    "    def predicate(self, train_X):\n",
    "        \"\"\"\n",
    "        预测\n",
    "        \"\"\"\n",
    "        logits = self.forward_progation(train_X)\n",
    "        one_hot = np.zeros_like(logits) # 生成logits大小的zero矩阵\n",
    "        one_hot[range(train_X.shape[0]),np.argmax(logits,axis=1)] = 1\n",
    "        return one_hot   \n",
    "\n",
    "    def fit(self,train_X, train_y, test_x, test_y):\n",
    "        \"\"\"\n",
    "        训练\n",
    "        \"\"\"\n",
    "        self.X = train_X\n",
    "        self.Y = train_y\n",
    "        n_y = train_y.shape[1]\n",
    "        m = train_X.shape[0]\n",
    "        \n",
    "        #初始化模型\n",
    "        self.init_model(train_X,n_classes=n_y,continue_train = False)\n",
    "        self.optimize(train_X, train_y, test_x, test_y,minibatch_size=600,learning_rate=0.01,num_iters=10)\n",
    "        \n",
    "        logits = self.predicate(train_X)\n",
    "        \n",
    "        accuracy = np.sum(np.argmax(logits,axis=1) == np.argmax(train_y,axis=1))/m\n",
    "        print(\"训练集的准确率为：%g\" %(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter : 0, batch : 0, acc 0.191667, loss 2.29274 \n",
      "iter : 0, batch : 1, acc 0.175, loss 2.29938 \n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "convNet = SimpleConvNet()\n",
    "convNet.fit(x_train,y_train,x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
